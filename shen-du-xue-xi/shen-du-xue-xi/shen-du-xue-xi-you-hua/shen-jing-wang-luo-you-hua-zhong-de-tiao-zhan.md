# 神经网络优化中的挑战

优化通常是一个极其困难的任务。传统的机器学习会小心设计目标函数和约束，以确保优化问题是凸的， 从而避免一般优化问题的复杂度。在训练神经网络时，我们肯定会遇到一般的非凸情况。 即使是凸优化，也并非没有任何问题。在这一节中，我们会总结几个训练深度模型时会涉及到的主要挑战。

## 病态

在优化凸函数时，会遇到一些挑战。这其中最突出的是Hessian矩阵 $$H$$ 的病态。这是数值优化、凸优化或其他形式的优化中普遍存在的问题。

病态问题一般被认为存在于神经网络训练过程中。病态体现在随机梯度下降会“卡”在某些情况，此时即使很小的更新步长也会增加代价函数。

代价函数的二阶泰勒级数展开预测梯度下降中的 $$-\epsilon g$$ 会增加

                                                                    $$\frac{1}{2} \epsilon^2 g^\top Hg - \epsilon g^\top g$$ 

到代价中。当 $$\frac{1}{2} \epsilon^2 g^\top Hg$$ 超过 $$\epsilon g^\top g$$ 时，梯度的病态会成为问题。判断病态是否不利于神经网络训练任务，我们可以监测平方梯度范数 $$g^\top g$$ 和 $$g^\top Hg$$ 。在很多情况中，梯度范数不会在训练过程中显著缩小，但是 $$g^\top Hg$$ 的增长会超过一个数量级。其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。如下图所示，成功训练的神经网络中，梯度显著增加

![](../../../.gitbook/assets/screenshot-from-2018-12-16-09-34-13.png)

尽管病态还存在于除了神经网络训练的其他情况中，有些适用于其他情况的解 决病态的技术并不适用于神经网络。例如，牛顿法在解决带有病态条件的Hessian矩阵的凸优化问题时，是一个非常优秀的工具，但是我们将会在以下小节中说明牛顿法运用到神经网络时需要很大的改动。

## 局部极小值

凸优化问题的一个突出特点是其可以简化为寻找一个局部极小点的问题。任何 一个局部极小点都是全局最小点。有些凸函数的底部是一个平坦的区域，而不是单 一的全局最小点,但该平坦区域中的任意点都是一个可以接受的解。优化一个凸问 题时，若发现了任何形式的临界点,我们都会知道已经找到了一个不错的可行解。

 对于非凸函数时，如神经网络，有可能会存在多个局部极小值。事实上，几乎所 有的深度模型基本上都会有非常多的局部极小值。然而，我们会发现这并不是主要 问题。

由于模型可辨识性\(model identifiability\)问题，神经网络和任意具有多个等效参数化潜变量的模型都会具有多个局部极小值。如果一个足够大的训练集可以唯 一确定一组模型参数，那么该模型被称为可辨认的。带有潜变量的模型通常是不可辨认的，因为通过相互交换潜变量我们能得到等价的模型。例如，考虑神经网络的第一层，我们可以交换单元 $$i$$ 和单元 $$j$$ 的传入权重向量、传出权重向量而得到等价的模型。如果神经网络有 $$m$$ 层,每层有 $$n$$ 个单元,那么会有 $$(n!)^m$$ 种排列隐藏单元的方式。这种不可辨认性被称为权重空间对称性\(weight space symmetry\)。

除了权重空间对称性，很多神经网络还有其他导致不可辨认的原因。例如，在任意整流线性网络或者maxout网络中，我们可以将传入权重和偏置扩大 $$\alpha$$ 倍,然 后将传出权重扩大 $$\frac{1}{\alpha}$$ 倍,而保持模型等价。这意味着，如果代价函数不包括如权重衰减这种直接依赖于权重而非模型输出的项，那么整流线性网络或者maxout网络的每一个局部极小点都在等价的局部极小值的 $$(m\times n)$$ 维双曲线上。

这些模型可辨识性问题意味着神经网络代价函数具有非常多、甚至不可数无限多的局部极小值。然而，所有这些由于不可辨识性问题而产生的局部极小值都有相 同的代价函数值。因此，这些局部极小值并非是非凸所带来的问题。

如果局部极小值相比全局最小点拥有很大的代价，局部极小值会带来很大的隐患。我们可以构建没有隐藏单元的小规模神经网络，其局部极小值的代价比全局最小点的代价大很多。如果具有很大代价的局部极小值是常见的，那么这将给基于梯度的优化算法带来极大的问题。

对于实际中感兴趣的网络，是否存在大量代价很高的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。多年来，大多数从业者认为局部极小值是困扰神经网络优化的常见问题。如今，情况有所变化。这个问题仍然是学术界的热点问题，但是学者们现在猜想，对于足够大的神经网络而言，大部分局部极小值都具有很小的代价函数，我们能不能找到真正的全局最小点并不重要，而是需要在参数空间中找到一个代价很小\(但不是最小\)的点。

很多从业者将神经网络优化中的所有困难都归结于局部极小值。我们鼓励从业者要仔细分析特定的问题。一种能够排除局部极小值是主要问题的检测方法是画出梯度范数随时间的变化。如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他形式的临界点。在高维空间中，很难明确证明局部极小值是导致问题的原因。许多并非局部极小值的结构也具有很小的梯度。

## 高原、鞍点和其他平坦区域

## 悬崖和梯度爆炸

## 长期依赖

## 非精确梯度

## 局部和全局结构的若对应

## 优化的理论限制

